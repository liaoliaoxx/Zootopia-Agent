import chromadb
import uuid
import json
import time
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer
from .prompts import NOTE_CONSTRUCTION_PROMPT, LINK_GENERATION_PROMPT, MEMORY_EVOLUTION_PROMPT
from utils import call_llm 

class AgenticMemorySystem:
    def __init__(self, agent_name: str, db_path: str = "./db"):
        self.agent_name = agent_name
        
        # 1. åˆå§‹åŒ–å‘é‡æ¨¡å‹ (è®ºæ–‡æ¨èä½¿ç”¨ dense retriever)
        # ç¬¬ä¸€æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ (çº¦ 80MB)
        print(f"[{self.agent_name}] æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹...")
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
        
        # 2. åˆå§‹åŒ– ChromaDB
        self.client = chromadb.PersistentClient(path=db_path)
        self.collection = self.client.get_or_create_collection(name=f"amem_{agent_name}")

    def _get_embedding(self, text: str) -> List[float]:
        return self.encoder.encode(text).tolist()

    def _parse_json_response(self, response: str) -> Dict:
        """é²æ£’çš„ JSON è§£æå™¨"""
        try:
            # å°è¯•æ¸…æ´— markdown æ ‡è®°
            clean_str = response.replace("```json", "").replace("```", "").strip()
            if not clean_str: return {}
            return json.loads(clean_str)
        except json.JSONDecodeError:
            print(f"âš ï¸ JSON Parsing Failed. Raw: {response[:50]}...")
            return {}

    def add_memory(self, content: str, timestamp: float = None):
        """
        A-MEM æ ¸å¿ƒå†™å…¥æµç¨‹ï¼šNote -> Link -> Evolve -> Store
        """
        if timestamp is None:
            timestamp = time.time()

        print(f"ğŸ§  [{self.agent_name}] æ­£åœ¨æ„å»ºç»“æ„åŒ–ç¬”è®° (A-MEM Processing)...")
        
        # === Phase 1: Note Construction (ç¬”è®°æ„é€ ) ===
        # è°ƒç”¨ LLM ç”Ÿæˆ Context, Keywords, Tags
        prompt = NOTE_CONSTRUCTION_PROMPT.format(content=content)
        
        # [ä¿®æ”¹ç‚¹ 1] ä¼ å…¥ system_prompt é˜²æ­¢æ¨¡å‹è§’è‰²æ‰®æ¼”
        raw_analysis = call_llm(
            prompt, 
            system_prompt="You are a helpful AI assistant specialized in text analysis and JSON generation.",
            json_mode=True
        )
        
        note_data = self._parse_json_response(raw_analysis)
        
        # å…œåº•é€»è¾‘ï¼šå¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
        context = note_data.get("context", content[:50])
        keywords = note_data.get("keywords", [])
        tags = note_data.get("tags", [])
        
        # æ„å»ºå¯Œæ–‡æœ¬ç”¨äº Embedding (Content + Context + Keywords)
        rich_text = f"{content} | Context: {context} | Keywords: {', '.join(keywords)}"
        embedding = self._get_embedding(rich_text)
        new_id = str(uuid.uuid4())

        # === Phase 2: Link Generation (åŠ¨æ€é“¾æ¥) ===
        # å…ˆæ£€ç´¢æœ€è¿‘çš„ k ä¸ªè®°å¿†
        neighbors = self.retrieve(query=rich_text, k=3)
        linked_ids = []
        
        if neighbors:
            neighbors_info = json.dumps([{ 'id': n['id'], 'content': n['content'], 'context': n['context'] } for n in neighbors], ensure_ascii=False)
            link_prompt = LINK_GENERATION_PROMPT.format(
                new_context=context, new_content=content, new_keywords=keywords, neighbors_info=neighbors_info
            )
            
            # [ä¿®æ”¹ç‚¹ 2] ä¼ å…¥ system_prompt
            link_res_raw = call_llm(
                link_prompt,
                system_prompt="You are a helpful AI assistant specialized in text analysis and JSON generation.",
                json_mode=True
            )
            link_res = self._parse_json_response(link_res_raw)
            linked_ids = link_res.get("linked_memory_ids", [])

        # === Phase 3: Memory Evolution (è®°å¿†è¿›åŒ–) ===
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°é‚»å±…çš„ Tags æˆ– Context
        if neighbors:
            evolve_prompt = MEMORY_EVOLUTION_PROMPT.format(new_content=content, neighbors_info=neighbors_info)
            
            # [ä¿®æ”¹ç‚¹ 3] ä¼ å…¥ system_prompt
            evolve_res_raw = call_llm(
                evolve_prompt,
                system_prompt="You are a helpful AI assistant specialized in text analysis and JSON generation.",
                json_mode=True
            )
            evolve_res = self._parse_json_response(evolve_res_raw)
            updates = evolve_res.get("updates", [])
            
            for update in updates:
                target_id = update.get("id")
                if target_id:
                    print(f"ğŸ§¬ [{self.agent_name}] è®°å¿†è¿›åŒ–: æ›´æ–°è®°å¿† {target_id[:4]} çš„ Context -> {str(update.get('new_context'))[:20]}...")
                    # æ³¨æ„ï¼šåœ¨çœŸå®ç”Ÿäº§ç¯å¢ƒä¸­ï¼ŒChromaDB æ›´æ–° metadata éœ€è¦è·å–åŸå§‹æ•°æ®å¹¶è¦†ç›–ï¼Œè¿™é‡Œä»…ä½œæ¼”ç¤ºæ‰“å°
                    # self.collection.update(...)

        # === Phase 4: Storage (è½åº“) ===
        self.collection.add(
            documents=[content],
            embeddings=[embedding],
            metadatas=[{
                "context": context,
                "keywords": ",".join(keywords),
                "tags": ",".join(tags),
                "linked_ids": ",".join(linked_ids),
                "timestamp": timestamp
            }],
            ids=[new_id]
        )
        print(f"âœ… è®°å¿†å·²å­˜å‚¨ [Tags: {tags}]")

    def retrieve(self, query: str, k: int = 5) -> List[Dict]:
        """
        æ£€ç´¢é€»è¾‘ï¼šEmbedding Search
        """
        query_embedding = self._get_embedding(query)
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=k
        )
        
        cleaned_results = []
        if results['ids']:
            for i in range(len(results['ids'][0])):
                # å¤„ç† metadata å¯èƒ½ä¸ºç©ºçš„æƒ…å†µ
                meta = results['metadatas'][0][i] if results['metadatas'][0][i] else {}
                cleaned_results.append({
                    "id": results['ids'][0][i],
                    "content": results['documents'][0][i],
                    "context": meta.get("context", ""),
                    "tags": meta.get("tags", "").split(","),
                    "score": results['distances'][0][i] if 'distances' in results else 0
                })
        return cleaned_results