import chromadb
import uuid
import json
import time
import os
from typing import List, Dict, Any
# å¼ºåˆ¶ä½¿ç”¨å›½å†…é•œåƒ
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
from sentence_transformers import SentenceTransformer
from .prompts import NOTE_CONSTRUCTION_PROMPT, LINK_GENERATION_PROMPT, MEMORY_EVOLUTION_PROMPT
from utils import call_llm 

class AgenticMemorySystem:
    def __init__(self, agent_name: str, db_path: str = "./db"):
        self.agent_name = agent_name
        
        print(f"[{self.agent_name}] æ­£åœ¨åŠ è½½ Embedding æ¨¡åž‹...")
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
        
        self.client = chromadb.PersistentClient(path=db_path)
        self.collection = self.client.get_or_create_collection(name=f"amem_{agent_name}")

    def _get_embedding(self, text: str) -> List[float]:
        return self.encoder.encode(text).tolist()

    def _parse_json_response(self, response: str) -> Dict:
        """é²æ£’çš„ JSON è§£æžå™¨ (å¢žå¼ºç‰ˆ)"""
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            try:
                if "```" in response:
                    start = response.find("{")
                    end = response.rfind("}") + 1
                    if start != -1 and end != 0:
                        return json.loads(response[start:end])
                
                start = response.find("{")
                end = response.rfind("}") + 1
                if start != -1 and end != 0:
                    return json.loads(response[start:end])
                
                print(f"âš ï¸ JSON Parsing Failed. Raw: {response}")
                return {}
            except Exception as e:
                print(f"âš ï¸ Critical JSON Error: {e}")
                return {}

    def add_memory(self, content: str, timestamp: float = None):
        """
        A-MEM æ ¸å¿ƒå†™å…¥æµç¨‹ï¼šNote -> Link -> Evolve -> Store
        """
        if timestamp is None:
            timestamp = time.time()

        print(f"ðŸ§  [{self.agent_name}] æ­£åœ¨æž„å»ºç»“æž„åŒ–ç¬”è®° (A-MEM Processing)...")
        
        # === Phase 1: Note Construction (ç¬”è®°æž„é€ ) ===
        prompt = NOTE_CONSTRUCTION_PROMPT.format(content=content)
        raw_analysis = call_llm(
            prompt, 
            system_prompt="You are a helpful AI assistant specialized in text analysis and JSON generation.",
            json_mode=True
        )
        note_data = self._parse_json_response(raw_analysis)
        
        context = note_data.get("context", content[:50])
        keywords = note_data.get("keywords", [])
        tags = note_data.get("tags", [])
        
        # æž„å»º Embedding
        rich_text = f"{content} | Context: {context} | Keywords: {', '.join(keywords)}"
        embedding = self._get_embedding(rich_text)
        new_id = str(uuid.uuid4())

        # === Phase 2: Link Generation (åŠ¨æ€é“¾æŽ¥) ===
        neighbors = self.retrieve(query=rich_text, k=3)
        linked_ids = []
        
        if neighbors:
            neighbors_info = json.dumps([{ 'id': n['id'], 'content': n['content'], 'context': n['context'] } for n in neighbors], ensure_ascii=False)
            link_prompt = LINK_GENERATION_PROMPT.format(
                new_context=context, new_content=content, new_keywords=keywords, neighbors_info=neighbors_info
            )
            
            link_res_raw = call_llm(
                link_prompt,
                system_prompt="You are a helpful AI assistant specialized in text analysis and JSON generation.",
                json_mode=True
            )
            link_res = self._parse_json_response(link_res_raw)
            linked_ids = link_res.get("linked_memory_ids", [])

        # === Phase 3: Memory Evolution (è®°å¿†è¿›åŒ– - çœŸå®žæ›´æ–°ç‰ˆ) ===
        if neighbors:
            evolve_prompt = MEMORY_EVOLUTION_PROMPT.format(new_content=content, neighbors_info=neighbors_info)
            evolve_res_raw = call_llm(
                evolve_prompt,
                system_prompt="You are a helpful AI assistant specialized in text analysis and JSON generation.",
                json_mode=True
            )
            evolve_res = self._parse_json_response(evolve_res_raw)
            updates = evolve_res.get("updates", [])
            
            for update in updates:
                target_id = update.get("id")
                if target_id:
                    # 1. å…ˆä»Žæ•°æ®åº“èŽ·å–å½“å‰çš„å®Œæ•´ Metadata (é˜²æ­¢è¦†ç›–ä¸¢å¤± timestamp ç­‰å­—æ®µ)
                    existing_record = self.collection.get(ids=[target_id])
                    
                    if existing_record and existing_record['metadatas']:
                        current_metadata = existing_record['metadatas'][0]
                        
                        # 2. å‡†å¤‡æ›´æ–°çš„æ•°æ®
                        new_context_val = update.get('new_context')
                        new_tags_val = update.get('new_tags')
                        
                        has_change = False
                        
                        # æ›´æ–° Context
                        if new_context_val and new_context_val != current_metadata.get('context'):
                            print(f"ðŸ§¬ [{self.agent_name}] è®°å¿†è¿›åŒ–: ID:{target_id[:4]} Context æ›´æ–° -> {str(new_context_val)[:30]}...")
                            current_metadata['context'] = new_context_val
                            has_change = True
                            
                        # æ›´æ–° Tags
                        if new_tags_val:
                            # ç¡®ä¿æ ¼å¼ç»Ÿä¸€ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
                            if isinstance(new_tags_val, list):
                                new_tags_str = ",".join(new_tags_val)
                            else:
                                new_tags_str = str(new_tags_val)
                                
                            if new_tags_str != current_metadata.get('tags'):
                                print(f"ðŸ·ï¸ [{self.agent_name}] æ ‡ç­¾è¿›åŒ–: ID:{target_id[:4]} Tags æ›´æ–° -> {new_tags_str}")
                                current_metadata['tags'] = new_tags_str
                                has_change = True
                        
                        # 3. æ‰§è¡ŒçœŸå®žçš„ Update æ“ä½œ
                        if has_change:
                            self.collection.update(
                                ids=[target_id],
                                metadatas=[current_metadata]
                                # æ³¨æ„ï¼šæˆ‘ä»¬åªæ›´æ–° metadataï¼Œä¿æŒåŽŸå§‹ embedding ä¸å˜ï¼Œ
                                # è¿™æ ·æ—¢ä¿ç•™äº†åŽŸå§‹è®°å¿†çš„â€œç‰©ç†ä½ç½®â€ï¼Œåˆæ›´æ–°äº†å®ƒçš„â€œè¯­ä¹‰è§£é‡Šâ€ã€‚
                            )

        # === Phase 4: Storage (è½åº“) ===
        self.collection.add(
            documents=[content],
            embeddings=[embedding],
            metadatas=[{
                "context": context,
                "keywords": ",".join(keywords),
                "tags": ",".join(tags),
                "linked_ids": ",".join(linked_ids),
                "timestamp": timestamp
            }],
            ids=[new_id]
        )
        print(f"âœ… è®°å¿†å·²å­˜å‚¨ [Tags: {tags}]")

    def retrieve(self, query: str, k: int = 5) -> List[Dict]:
        query_embedding = self._get_embedding(query)
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=k
        )
        
        cleaned_results = []
        if results['ids']:
            for i in range(len(results['ids'][0])):
                meta = results['metadatas'][0][i] if results['metadatas'][0][i] else {}
                cleaned_results.append({
                    "id": results['ids'][0][i],
                    "content": results['documents'][0][i],
                    "context": meta.get("context", ""),
                    "tags": meta.get("tags", "").split(","),
                    "score": results['distances'][0][i] if 'distances' in results else 0
                })
        return cleaned_results